{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cb198a-50b5-490a-94ab-07dd70565468",
   "metadata": {},
   "source": [
    "### Verificar se a biblioteca da OCI está baixada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fc3dce2-1960-4361-ab3a-ab1ca9bbea4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.54.2\n"
     ]
    }
   ],
   "source": [
    "!oci --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1a1af-fd20-4668-b917-3b0b2e01872b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Caso não esteja, rodar o comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034546a-507b-4c1e-b0d9-6bcfe30d8e6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install oci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9385bf-5301-47ab-8ce4-202948530737",
   "metadata": {},
   "source": [
    "### Configure um arquivo com credenciais API Key\n",
    "- No terminal rode o comado: \n",
    "> oci setup config\n",
    "- Adicionar OCID (no seu perfil > user_setings)\n",
    "- Adicionar Tenanty OCID (no seu perfil > tenancy)\n",
    "- Deixar pastas padrão\n",
    "- Configurar chave pública(perfil > user_settings > tokens and keys > add api key > paste a public key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d2508-4386-45ae-882e-2df1f77d4f8a",
   "metadata": {},
   "source": [
    "### Código direto do code examples no OCI GenAI\n",
    "- Llama 3.3 70B \n",
    "\n",
    "⚠️⚠️⚠️⚠️⚠️ O código não funciona se você não alterar para o seu compartment_id e conferir model_id ⚠️⚠️⚠️⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5315852-431e-4c0b-aeab-715de022bd31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"O que é uma llama?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61e9803-5d21-43b4-a2f2-991d7d24aa86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Chat Result**************************\n",
      "{\n",
      "    \"response\": \"Uma llama é um mamífero da família dos camelídeos, nativo da América do Sul, principalmente nos Andes. É um animal herbívoro, conhecido por sua pelagem macia e sua habilidade de carregar cargas. As lhamas são frequentemente utilizadas como animais de carga e também são valorizadas por sua lã. Elas são inteligentes e sociais, vivendo em rebanhos no seu habitat natural.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import oci\n",
    "import json\n",
    "\n",
    "# Setup basic variables\n",
    "# Auth Config\n",
    "# TODO: Please update config profile name and use the compartmentId that has policies grant permissions for using Generative AI Service\n",
    "compartment_id = \"ocid1.tenancy.oc1..aaaaaaaakzuzfystmhyj7je5x7ymwd3ofd7wb6rtsfrx7nvbo272vta3rwna\"\n",
    "CONFIG_PROFILE = \"SAOPAULO\"\n",
    "config = oci.config.from_file('~/.oci/config', CONFIG_PROFILE)\n",
    "\n",
    "# Service endpoint\n",
    "endpoint = \"https://inference.generativeai.sa-saopaulo-1.oci.oraclecloud.com\"\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint)\n",
    "chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "\n",
    "content = oci.generative_ai_inference.models.TextContent()\n",
    "content.text = f\"{user_input}\"\n",
    "message = oci.generative_ai_inference.models.Message()\n",
    "message.role = \"USER\"\n",
    "message.content = [content]\n",
    "\n",
    "chat_request = oci.generative_ai_inference.models.GenericChatRequest()\n",
    "chat_request.api_format = oci.generative_ai_inference.models.BaseChatRequest.API_FORMAT_GENERIC\n",
    "chat_request.messages = [message]\n",
    "chat_request.max_tokens = 600\n",
    "chat_request.temperature = 1\n",
    "chat_request.frequency_penalty = 0\n",
    "chat_request.presence_penalty = 0\n",
    "chat_request.top_p = 0.75\n",
    "\n",
    "chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"ocid1.generativeaimodel.oc1.sa-saopaulo-1.amaaaaaask7dceyarsn4m6k3aqvvgatida3omyprlcs3alrwcuusblru4jaa\")\n",
    "chat_detail.chat_request = chat_request\n",
    "chat_detail.compartment_id = compartment_id\n",
    "\n",
    "chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "# Print result\n",
    "print(\"**************************Chat Result**************************\")\n",
    "choices = chat_response.data.chat_response.choices\n",
    "if choices and len(choices) > 0:\n",
    "    response_text = choices[0].message.content[0].text\n",
    "else:\n",
    "    response_text = \"Nenhuma resposta foi gerada pelo modelo.\"\n",
    "# Exibindo a resposta no console\n",
    "print(json.dumps({\"response\": response_text}, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9199e4-f459-4168-a3d3-22fb9add6377",
   "metadata": {},
   "source": [
    "### Multimodal\n",
    "- Llama 3.2 90B - processando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221ca58d-2430-40b6-a0d2-d3b69fe18282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuração da mensagem do sistema e do usuário\n",
    "system_msg = \"\"\"Você é um especialista em analisar imagens\"\"\"\n",
    "user_msg = \"o que tem na imagem? \"\n",
    "image_content = \"demo-image.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef8a8b-3d72-4806-aec4-5973cc91ec52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"response\": \"Claro, posso analisar a imagem e fornecer informações sobre ela. A imagem mostra uma mulher com cabelos pretos e óculos, vestindo uma jaqueta vermelha, ao lado de um animal que parece ser uma lhama. A cena parece ser em um ambiente natural, com árvores e uma cerca de madeira ao fundo. A mulher está sorrindo e olhando para a câmera, enquanto a lhama está olhando para a frente. A imagem parece ser uma selfie, pois a mulher está segurando a câmera com uma das mãos. A qualidade da imagem é boa, com cores vivas e detalhes claros. Não há nada anormal ou suspeito na imagem.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# Copyright (c) 2023, Oracle and/or its affiliates.  All rights reserved.\n",
    "# This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.\n",
    "\n",
    "##########################################################################\n",
    "# chat_demo.py\n",
    "# Supports Python 3\n",
    "##########################################################################\n",
    "# Info:\n",
    "# Get texts from LLM model for given prompts using OCI Generative AI Service.\n",
    "##########################################################################\n",
    "# Application Command line(no parameter needed)\n",
    "# python chat_demo.py\n",
    "##########################################################################\n",
    "import oci\n",
    "\n",
    "# Setup basic variables\n",
    "# Auth Config\n",
    "# TODO: Please update config profile name and use the compartmentId that has policies grant permissions for using Generative AI Service\n",
    "compartment_id = \"ocid1.compartment.oc1..aaaaaaaa3x7n7wwfnghe4imvt3niwo76wgqv6ecn2iadiwoph73jjowbhbna\"\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('~/.oci/config', CONFIG_PROFILE)\n",
    "\n",
    "# Service endpoint\n",
    "endpoint = \"https://inference.generativeai.sa-saopaulo-1.oci.oraclecloud.com\"\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "\n",
    "###### Código adicionado para suporte a imagem ######\n",
    "import base64\n",
    "import json\n",
    "\n",
    "\n",
    "# Transforma a imagem em base64\n",
    "try:\n",
    "    with open(image_content, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "    base64_image = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "    base64_image_url = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "except oci.exceptions.ServiceError as e:\n",
    "    raise ValueError(f\"Erro ao acessar a imagem\")\n",
    "    \n",
    "\n",
    "# Configuração do conteúdo da imagem\n",
    "imagem = oci.generative_ai_inference.models.ImageContent()\n",
    "image_url_content = oci.generative_ai_inference.models.ImageUrl()\n",
    "image_url_content.url = base64_image_url\n",
    "imagem.image_url = image_url_content\n",
    "\n",
    "# Configuração do conteúdo de texto\n",
    "texto = oci.generative_ai_inference.models.TextContent()\n",
    "texto.text = \"O que tem nesta imagem?\"\n",
    "\n",
    "# Configuração da mensagem do usuário\n",
    "message = oci.generative_ai_inference.models.UserMessage()\n",
    "message.role = \"USER\"\n",
    "message.content = [texto, imagem]\n",
    "#################################################################\n",
    "\n",
    "chat_request = oci.generative_ai_inference.models.GenericChatRequest()\n",
    "chat_request.api_format = oci.generative_ai_inference.models.BaseChatRequest.API_FORMAT_GENERIC\n",
    "chat_request.messages = [message]\n",
    "chat_request.max_tokens = 600\n",
    "chat_request.temperature = 1\n",
    "chat_request.frequency_penalty = 0\n",
    "chat_request.presence_penalty = 0\n",
    "chat_request.top_p = 0.75\n",
    "\n",
    "\n",
    "\n",
    "chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"ocid1.generativeaimodel.oc1.sa-saopaulo-1.amaaaaaask7dceyalwceqwzlywqqxfzz3grpzjr42fej5qlybhu2d666oz4q\")\n",
    "chat_detail.chat_request = chat_request\n",
    "chat_detail.compartment_id = compartment_id\n",
    "\n",
    "chat_response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "# Print result\n",
    "print(\"**************************Chat Result**************************\")\n",
    "print(vars(chat_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04148a49-48a5-4408-aeed-7fce9183fcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
