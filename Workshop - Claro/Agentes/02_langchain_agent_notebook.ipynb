{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Agente LangChain - Agente de Análise de Cliente para Marketing Telco\n",
    "\n",
    " Este notebook implementa um agente que analisa informações de clientes para gerar ações de marketing.\n",
    "\n",
    "Este agente utiliza sinais gerados por modelos de IA tradicional (churn e valor mensal) para gerar insights de negócio para Marketing.\n",
    "\n",
    "Ele não toma decisões automáticas.\n",
    "Ele explica contextos e sugere caminhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~etuptools (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~etuptools (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain-oci in /opt/conda/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.11/site-packages (1.2.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: aiohttp>=3.12.14 in /opt/conda/lib/python3.11/site-packages (from langchain-oci) (3.13.3)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-oci) (1.2.7)\n",
      "Requirement already satisfied: langchain-openai<2.0.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-oci) (1.1.7)\n",
      "Requirement already satisfied: oci>=2.161.0 in /opt/conda/lib/python3.11/site-packages (from langchain-oci) (2.163.0)\n",
      "Requirement already satisfied: oci-openai>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from langchain-oci) (1.0.0)\n",
      "Requirement already satisfied: openai>=2.6.1 in /opt/conda/lib/python3.11/site-packages (from langchain-oci) (2.14.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain-oci) (2.12.4)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-oci) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/conda/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-oci) (0.6.2)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-oci) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-oci) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-oci) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-oci) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-oci) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-oci) (3.0.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from langchain-openai<2.0.0,>=1.1.0->langchain-oci) (0.12.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/conda/lib/python3.11/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/conda/lib/python3.11/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-oci) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-oci) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-oci) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai>=2.6.1->langchain-oci) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from openai>=2.6.1->langchain-oci) (0.11.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai>=2.6.1->langchain-oci) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai>=2.6.1->langchain-oci) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2->langchain-oci) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2->langchain-oci) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2->langchain-oci) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai<2.0.0,>=1.1.0->langchain-oci) (2025.11.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp>=3.12.14->langchain-oci) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp>=3.12.14->langchain-oci) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp>=3.12.14->langchain-oci) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp>=3.12.14->langchain-oci) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp>=3.12.14->langchain-oci) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp>=3.12.14->langchain-oci) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp>=3.12.14->langchain-oci) (1.22.0)\n",
      "Requirement already satisfied: cryptography<46.0.0,>=3.2.1 in /opt/conda/lib/python3.11/site-packages (from oci>=2.161.0->langchain-oci) (45.0.7)\n",
      "Requirement already satisfied: pyOpenSSL<=25.1.0,>=17.5.0 in /opt/conda/lib/python3.11/site-packages (from oci>=2.161.0->langchain-oci) (25.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from oci>=2.161.0->langchain-oci) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2016.10 in /opt/conda/lib/python3.11/site-packages (from oci>=2.161.0->langchain-oci) (2025.2)\n",
      "Requirement already satisfied: circuitbreaker<3.0.0,>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from oci>=2.161.0->langchain-oci) (2.1.3)\n",
      "Requirement already satisfied: cffi>=1.14 in /opt/conda/lib/python3.11/site-packages (from cryptography<46.0.0,>=3.2.1->oci>=2.161.0->langchain-oci) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.5.3->oci>=2.161.0->langchain-oci) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.14->cryptography<46.0.0,>=3.2.1->oci>=2.161.0->langchain-oci) (2.21)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-oci) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-oci) (2.5.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~etuptools (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~etuptools (/opt/conda/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain-oci langchain scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ferramenta que busca informações do cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.tools import tool\n",
    "df = pd.read_csv(\"telco_churn.csv\")\n",
    "\n",
    "@tool\n",
    "def buscar_cliente(id_cliente: int) -> dict:\n",
    "    \"\"\"\n",
    "    Retorna os dados de um cliente Telco a partir do id_cliente.\n",
    "    \"\"\"\n",
    "    cliente = df[df[\"id_cliente\"] == id_cliente]\n",
    "    cliente = cliente.drop(columns=[\"churn\"])\n",
    "    \n",
    "    if cliente.empty:\n",
    "        return \"Cliente não está na base\"\n",
    "    \n",
    "    return cliente.to_dict(orient=\"records\")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ferramenta que calcula a probabilidade de churn do cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "X = df.drop(columns=['id_cliente', 'churn'])\n",
    "y = df['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "@tool\n",
    "def prever_churn_cliente(id_cliente: int) -> str:\n",
    "    \"\"\"\n",
    "    Retorna a probabilidade de churn de um cliente específico.\n",
    "    \"\"\"\n",
    "    # Buscar cliente\n",
    "    cliente = df[df[\"id_cliente\"] == id_cliente]\n",
    "\n",
    "    if cliente.empty:\n",
    "        return json.dumps({\n",
    "            \"erro\": \"Cliente não encontrado\"\n",
    "        })\n",
    "\n",
    "    # Preparar features (mesma ordem do treino!)\n",
    "    X_cliente = cliente[X.columns]\n",
    "\n",
    "    # Escalar\n",
    "    X_cliente_scaled = scaler.transform(X_cliente)\n",
    "\n",
    "    # Predizer probabilidade\n",
    "    churn_prob = model.predict_proba(X_cliente_scaled)[0][1]\n",
    "\n",
    "    # Retorno estruturado (ideal para agente)\n",
    "    return json.dumps({\n",
    "        \"id_cliente\": int(id_cliente),\n",
    "        \"probabilidade_churn\": round(float(churn_prob), 2),\n",
    "        \"interpretacao\": (\n",
    "            \"Risco alto de churn\" if churn_prob > 0.7\n",
    "            else \"Risco médio de churn\" if churn_prob > 0.4\n",
    "            else \"Risco baixo de churn\"\n",
    "        )\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configuração de autenticação de OCI SDK\n",
    "    - https://github.com/oracle/oci-python-sdk/blob/master/examples/configuration_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Importando bibliotecas e callback handler\n",
    " Callback é uma configuração para conseguir visualizar passos internos do caminho do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_oci.chat_models import ChatOCIGenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.messages import AIMessage\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "class CleanAgentCallback(BaseCallbackHandler):\n",
    "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
    "        tool_name = serialized.get(\"name\", \"unknown_tool\")\n",
    "        print(f\"\\n TOOL: {tool_name}\")\n",
    "        print(f\"   ↳ input: {input_str}\")\n",
    "\n",
    "    def on_tool_end(self, output, **kwargs):\n",
    "        print(\"TOOL RESULT:\")\n",
    "\n",
    "        content = getattr(output, \"content\", output)\n",
    "\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "            print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "        except Exception:\n",
    "            print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração do modelo LLM do agente (Cerebro)\n",
    "Observe que a região deve ser a mesma que a da sua chave, se precisar, altere abaixo.\n",
    "\n",
    "#### Cohere \n",
    "- \"cohere.command-a-03-2025\", (sp)\n",
    "- \"cohere.command-latest\", (sp)\n",
    "- \"cohere.command-plus-latest\", (sp)\n",
    "- \"cohere.command-r-08-2024\", (sp)\n",
    "- \"cohere.command-r-plus-08-2024\", (sp)\n",
    "\n",
    "#### Google Gemini (apenas us)\n",
    "- \"google.gemini-2.5-pro\", \n",
    "- \"google.gemini-2.5-flash\",\n",
    "- \"google.gemini-2.5-flash-lite\",\n",
    "\n",
    "#### Meta Llama\n",
    "- \"meta.llama-3.1-405b-instruct\", (sp)\n",
    "- \"meta.llama-3.2-90b-vision-instruct\", (sp)\n",
    "- \"meta.llama-3.3-70b-instruct\", (sp)\n",
    "- \"meta.llama-4-maverick-17b-128e-instruct-fp8\",\n",
    "- \"meta.llama-4-scout-17b-16e-instruct\",\n",
    "\n",
    "#### OpenAI OSS (apenas us)\n",
    "- \"openai.gpt-oss-120b\",\n",
    "- \"openai.gpt-oss-20b\",\n",
    "\n",
    "#### xAI Grok (apenas us)\n",
    "- \"xai.grok-code-fast-1\",\n",
    "- \"xai.grok-4-fast-non-reasoning\",\n",
    "- \"xai.grok-4-fast-reasoning\",\n",
    "- \"xai.grok-4\",\n",
    "- \"xai.grok-3\",\n",
    "- \"xai.grok-3-mini\",\n",
    "- \"xai.grok-3-mini-fast\",\n",
    "- \"xai.grok-3-fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-chicago-1\"\n",
    "model_name = \"xai.grok-3-mini-fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOCIGenAI(\n",
    "  model_id=model_name,\n",
    "  service_endpoint=f\"https://inference.generativeai.{region}.oci.oraclecloud.com\",\n",
    "  compartment_id=\"ocid1.compartment.oc1..aaaaaaaa3x7n7wwfnghe4imvt3niwo76wgqv6ecn2iadiwoph73jjowbhbna\",\n",
    "  provider=\"meta\",\n",
    "  model_kwargs={\n",
    "    \"temperature\": 0.3, \n",
    "    \"max_tokens\": 800,   \n",
    "    \"top_p\": 0.8, \n",
    "  },\n",
    "  auth_type=\"API_KEY\",\n",
    "  auth_profile=\"DEFAULT\",\n",
    "  auth_file_location = \"./config\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando ferramentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    buscar_cliente,\n",
    "    prever_churn_cliente\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt do agente e criação do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Você é um analista sênior de Marketing na Claro.\n",
    "\n",
    "Você deve, quando necessário:\n",
    "- Consultar os dados do cliente\n",
    "- Consultar a probabilidade de churn via ferramenta apropriada\n",
    "- Analisar o contexto do cliente (uso, valor, histórico)\n",
    "- Construir uma proposta de marketing coerente e explicável\n",
    "- Caso a proposta seja realizar uma medida de retenção, gerar uma proposta de contato para o cliente ao final.\n",
    "\n",
    "IMPORTANTE:\n",
    "- NÃO use regras fixas\n",
    "- NÃO invente informações! Use apenas os dados captados nas ferramentas para fazer sua analise.\n",
    "- NÃO retorne apenas rótulos como \"alto\" ou \"baixo\"\n",
    "- Sempre explique o raciocínio por trás da proposta\n",
    "\n",
    "Você tem acesso às seguintes ferramentas:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use EXATAMENTE o formato abaixo:\n",
    "\n",
    "Question: a pergunta recebida\n",
    "Thought: o que você precisa analisar e quais informações buscar\n",
    "Action: a ferramenta que você irá usar\n",
    "Action Input: o input da ferramenta\n",
    "Observation: o resultado retornado pela ferramenta\n",
    "Thought: análise integrada dos dados e sinais obtidos\n",
    "Final Answer: \n",
    "- Um resumo do perfil do cliente\n",
    "- A probabilidade de churn (em percentual)\n",
    "- Uma proposta de ação de marketing contextualizada\n",
    "- A justificativa da proposta, em linguagem de negócio\n",
    "\n",
    "Sempre responda em português do Brasil.\n",
    "\n",
    "Begin!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "agent = create_agent(llm, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processar resposta final para deixar mais bonito (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_output(agent_response):\n",
    "    final_text = None\n",
    "    total_tokens = None\n",
    "\n",
    "    for msg in reversed(agent_response[\"messages\"]):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            if msg.content and final_text is None:\n",
    "                if \"Final Answer:\" in msg.content:\n",
    "                    final_text = msg.content.split(\"Final Answer:\", 1)[1].strip()\n",
    "                else:\n",
    "                    final_text = msg.content.strip()\n",
    "\n",
    "            if total_tokens is None:\n",
    "                total_tokens = (\n",
    "                    msg.response_metadata.get(\"total_tokens\")\n",
    "                    or msg.additional_kwargs.get(\"total_tokens\")\n",
    "                )\n",
    "\n",
    "        if final_text and total_tokens:\n",
    "            break\n",
    "\n",
    "    return final_text, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução prática do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TOOL: buscar_cliente\n",
      "   ↳ input: {'id_cliente': 1024}\n",
      "TOOL RESULT:\n",
      "{\n",
      "  \"id_cliente\": 1024,\n",
      "  \"tempo_contrato_meses\": 3,\n",
      "  \"valor_mensal\": 185,\n",
      "  \"uso_dados_gb\": 5,\n",
      "  \"qtd_reclamacoes\": 2,\n",
      "  \"atraso_pagamento\": 1,\n",
      "  \"recebeu_campanha\": 0\n",
      "}\n",
      "\n",
      " TOOL: prever_churn_cliente\n",
      "   ↳ input: {'id_cliente': 1024}\n",
      "TOOL RESULT:\n",
      "{\n",
      "  \"id_cliente\": 1024,\n",
      "  \"probabilidade_churn\": 0.97,\n",
      "  \"interpretacao\": \"Risco alto de churn\"\n",
      "}\n",
      "\n",
      "Final answer:\n",
      "- **Resumo do perfil do cliente:** O cliente com ID 1024 tem um contrato de apenas 3 meses, indicando um relacionamento inicial e potencialmente frágil com a Claro. Ele paga R$ 185 mensalmente, mas utiliza apenas 5 GB de dados, o que sugere subutilização do plano e pouca interação com os serviços oferecidos. Além disso, há 2 reclamações registradas e 1 atraso no pagamento, o que pode refletir insatisfação ou desafios financeiros, e ele nunca recebeu campanhas de marketing, o que representa uma lacuna em nossos esforços de engajamento.\n",
      "- **Probabilidade de churn:** 97%, indicando um risco extremamente alto de o cliente cancelar o serviço no curto prazo, com base na análise preditiva.\n",
      "- **Proposta de ação de marketing:** Dado o alto risco de churn, recomendo uma estratégia de retenção proativa e personalizada, como oferecer um upgrade gratuito para um plano com mais dados (por exemplo, dobrando o limite para 10 GB nos próximos 3 meses) e um desconto de 15% na fatura seguinte, vinculado a uma avaliação de satisfação. Além disso, inclua uma oferta de conteúdo exclusivo, como acesso a serviços de streaming parceiros, para incentivar o uso diário e demonstrar o valor da Claro.\n",
      "- **Justificativa da proposta:** Essa proposta é fundamentada na análise integrada dos dados, que revela um cliente com baixo engajamento e sinais de insatisfação (reclamações e atraso), o que contribui para a probabilidade de churn de 97%. Em termos de negócio, investir em retenção agora é mais eficiente do que adquirir novos clientes, pois o custo de manutenção é menor e pode converter esse risco em oportunidade de fidelização. Por exemplo, ao personalizar a oferta com base no baixo uso de dados, aumentamos a percepção de valor e o engajamento, potencialmente elevando o lifetime value (LTV) do cliente. Essa abordagem evita a perda iminente e alinha com métricas de marketing como redução de churn e aumento da retenção, que impactam diretamente a receita recorrente.\n",
      "\n",
      "Para avançar, sugiro que o time de retenção realize um contato imediato via ligação ou e-mail personalizado, destacando os benefícios da oferta e convidando-o a discutir suas necessidades, com o objetivo de resolver as reclamações e fortalecer o relacionamento.\n",
      "\n",
      "Metadados:\n",
      "total tokens: 2194\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"\"\"\n",
    "Analise o cliente de id 1024.\n",
    "Explique o comportamento dele e aponte possíveis riscos ou oportunidades\n",
    "para ações de marketing.\n",
    "\"\"\"\n",
    "\n",
    "callback = CleanAgentCallback()\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": pergunta},\n",
    "    config={\"callbacks\": [callback]}\n",
    ")\n",
    "\n",
    "final_answer, total_tokens = extract_final_output(response)\n",
    "\n",
    "print(\"\\nFinal answer:\")\n",
    "print(final_answer)\n",
    "\n",
    "print(\"\\nMetadados:\")\n",
    "print(f\"total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo cenário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TOOL: buscar_cliente\n",
      "   ↳ input: {'id_cliente': 1010}\n",
      "TOOL RESULT:\n",
      "{\n",
      "  \"id_cliente\": 1010,\n",
      "  \"tempo_contrato_meses\": 60,\n",
      "  \"valor_mensal\": 90,\n",
      "  \"uso_dados_gb\": 35,\n",
      "  \"qtd_reclamacoes\": 0,\n",
      "  \"atraso_pagamento\": 0,\n",
      "  \"recebeu_campanha\": 1\n",
      "}\n",
      "\n",
      " TOOL: prever_churn_cliente\n",
      "   ↳ input: {'id_cliente': 1010}\n",
      "TOOL RESULT:\n",
      "{\n",
      "  \"id_cliente\": 1010,\n",
      "  \"probabilidade_churn\": 0.01,\n",
      "  \"interpretacao\": \"Risco baixo de churn\"\n",
      "}\n",
      "\n",
      "Final answer:\n",
      "- **Resumo do perfil do cliente:** O cliente 1010 é um assinante leal da Claro, com 60 meses de contrato, o que indica uma relação de longo prazo. Ele gasta R$90 mensalmente e utiliza cerca de 35 GB de dados por mês, sem nenhuma reclamação registrada ou atrasos em pagamentos. Além disso, já recebeu uma campanha promocional anteriormente, o que sugere engajamento prévio com ofertas da empresa.  \n",
      "- **A probabilidade de churn:** 1% (baseada na previsão obtida, que indica um risco baixo).  \n",
      "- **Proposta de ação de marketing contextualizada:** Recomendo uma ação de upselling, como oferecer um upgrade para um plano mais avançado que inclua mais dados (por exemplo, de 35 GB para 50 GB) ou serviços adicionais, como um pacote de streaming, com um desconto especial de 10% por 6 meses. Essa oferta pode ser enviada via e-mail ou SMS personalizado, destacando os benefícios com base no histórico de uso do cliente.  \n",
      "- **Justificativa da proposta, em linguagem de negócio:** Essa proposta é fundamentada na análise integrada dos dados, que mostram um cliente altamente valioso e de baixo risco de churn, com um LTV (Lifetime Value) potencialmente alto devido ao longo tempo de contrato e pagamentos regulares. Ao investir em upselling para um cliente satisfeito e engajado, a Claro pode aumentar a receita média por usuário (ARPU) sem custos significativos de aquisição, promovendo uma experiência personalizada que reforça a lealdade e aproveita o baixo risco para gerar crescimento orgânico. Essa abordagem é estratégica, pois evita ações reativas de retenção e foca em oportunidades de expansão com base no comportamento histórico do cliente.\n",
      "\n",
      "Metadados:\n",
      "total tokens: 1798\n"
     ]
    }
   ],
   "source": [
    "pergunta = f\"cliente 1010\"\n",
    "response = agent.invoke(\n",
    "    {\"messages\": pergunta},\n",
    "    config={\"callbacks\": [callback]}\n",
    ")\n",
    "\n",
    "final_answer, total_tokens = extract_final_output(response)\n",
    "\n",
    "print(\"\\nFinal answer:\")\n",
    "print(final_answer)\n",
    "\n",
    "print(\"\\nMetadados:\")\n",
    "print(f\"total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final answer:\n",
      "Desculpe, mas eu sou um analista sênior de Marketing na Claro, especializado em assuntos relacionados a serviços de telecomunicações, análise de clientes e estratégias de retenção. Sua pergunta sobre como fazer um bolo de chocolate não está dentro do escopo do meu papel.\n",
      "\n",
      "Se você tiver dúvidas sobre planos Claro, análise de uso de dados, ou qualquer coisa relacionada aos nossos serviços, é só perguntar! Caso contrário, posso te recomendar buscar essa informação em sites de receitas ou vídeos tutoriais. Como posso ajudar com algo relacionado à Claro?\n",
      "\n",
      "Metadados:\n",
      "total tokens: 1061\n"
     ]
    }
   ],
   "source": [
    "pergunta = f\"como eu faço um bolo de chocolate?\"\n",
    "response = agent.invoke(\n",
    "    {\"messages\": pergunta},\n",
    "    config={\"callbacks\": [callback]}\n",
    ")\n",
    "\n",
    "final_answer, total_tokens = extract_final_output(response)\n",
    "\n",
    "print(\"\\nFinal answer:\")\n",
    "print(final_answer)\n",
    "\n",
    "print(\"\\nMetadados:\")\n",
    "print(f\"total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
